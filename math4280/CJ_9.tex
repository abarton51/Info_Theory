\documentclass{article}
\usepackage[a4paper, top=2cm, bottom=2cm, left=2cm, right=2cm]%
{geometry}
\usepackage{graphicx}
\usepackage{amsthm, amsmath, amssymb, tikz}
\usepackage{enumitem}
\usepackage{xifthen}
\usepackage{mathtools}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage[colorlinks=true,
linkcolor=blue,citecolor=blue,
urlcolor=blue]{hyperref}

\usetikzlibrary{calc,shapes, backgrounds}


\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{\color{red} Conjecture}
\newtheorem{proposition}{Proposition}
\newtheorem{open}{\color{red} Open Question}


%Question template----------------------------------------
\def\indented#1{\list{}{}\item[]}
\let\indented=\endlist

\newcounter{questionCounter}
\newcounter{partCounter}[questionCounter]
\newenvironment{question}[2][\arabic{questionCounter}]{%
    \setcounter{partCounter}{0}%
    \vspace{.25in} \hrule \vspace{0.5em}%
        \noindent{\bf \large#2}%
    \vspace{0.5em} \hrule \vspace{.10in}%
    \addtocounter{questionCounter}{1}%
}{}
\renewenvironment{part}[1][\alph{partCounter}]{%
    \addtocounter{partCounter}{1}%
    \vspace{.10in}%
    \begin{indented}%
       {\bf (#1)} %
}{\end{indented}}



\begin{document}


\begin{center}
{\Huge Class Journal 9}\\
\end{center}

\vspace{5mm}

\noindent {\large Lecture Handout 9:}

\vspace{5mm}

\noindent {\large Topic of the lecture: Entropy rate of random walk on graphs \& 2nd Law of Thermodynamics & Functions of Markov Chains}

\begin{question}{Main Theorems/Ideas in the lecture}
We went over how to determine the probability transition matrix of a random walk, specifically the equiprobable case, and how to determine its stationary distribution if it exists.

We discussed how, typically, a random walk can be modeled as a Markov chain, although not always.

We then used the equation for the entropy rate of a Markov Chain in order to determine th entropy rate of this random walk.

We then discussed the 2nd law of thermodynamics. We discussed how to interpret entropy from this perspective rather than the information theory perspective and how they are similar.

The Second law of thermodynamics states: the entropy of an isolated system is nondecreasing.

We then discussed 4 important interpreations of the second law of thermodynamics.
\begin{enumerate}
    \item Relative entropy $D(\mu_n || \mu'_n)$ decreases with $n$. 
    \item Relative entropy $D(\mu_n || \mu'_n)$ decreases with $n$ where $\mu$ is the stationary distribution.
    \item Entropy increases if the stationary distribution is uniform.
    \item $H(X_n|X_1)$ increases with $n$ for a stationary Markov process.

We then went on to discuss stochastic processes formed from functions of each state of a stochastic process.

Let $X_1, \ldots, X_n$ be a stationary Markov chain. Let $Y_i = \phi(X_i)$ where $\phi$ is a function. Then
\begin{gather*}
    H(Y_n|Y_{n-1}, \ldots, Y_2, X_1)\leq H(\mathcal{Y})
\end{gather*}
But they do get close. 

$H(Y_n|Y_{n-1}, \ldots, Y_1) - H(Y_n|Y_{n-1}, \ldots, Y_2, X_1) \to 0$ as $n\to \infty$.

Combining these ideas, with the overarching idea that the input $X_i$ determines $\phi(Y_i)$, we concluded the lecture with the theorem,
\begin{gather*}
    H(Y_n|Y_{n-1}, \ldots, Y_2, Y_1, X_1)\leq H(\mathcal{Y})\leq H(Y_{n}|Y_{n-1}, \ldots,Y_2,  Y_1)
\end{gather*}
and 
\begin{gather*}
    \lim_{n\to\infty}H(Y_n|Y_{n-1}, \ldots, Y_1, X_1) = H(\mathcal{Y}) = \lim_{n\to\infty}H(Y_{n}|Y_{n-1}, \ldots,Y_2,  Y_1)
\end{gather*}
\end{enumerate}

\vspace{5cm}

\end{question}

\begin{question}{Materials that I find confusing or need more clarification/examples}
I need to go back over my notes and possibly a discussion over office hours about the statement
"‘Isolated System’: modelled as a Markov chain. Implicit in this assumption is the notion of an overall state
of the system and the fact that knowing the present state, the future of the system is independent of the
past"

I think I understand the 2nd law and the interpretations well. But I'm not seeing how this statement relates to the 2nd law that well. Or rather, I see its relation, but why is it "implicit"?
\vspace{3cm}

\end{question}

\begin{question}{Class journal questions (if there are any in the lecture handout)}
No Journal Questions.
\end{question}

\vspace*{6cm}

\begin{question}{Additional comments/suggestions about the lecture or the instructor}
None, thank you.
% {\color{blue}

% \begin{enumerate}
%     \item Said too many `Ok's.
%     \item Speak slower.
% \end{enumerate}

% }

\end{question}




\end{document}
